# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é AI

## –í–≤–µ–¥–µ–Ω–∏–µ

–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö, –æ–ø–∏—Å–∞–Ω–∏–π –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

## –û–±–∑–æ—Ä –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π

–° –ø–æ–º–æ—â—å—é AI –º–æ–∂–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—é –æ–ø–∏—Å–∞–Ω–∏–π –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ç–µ–º
- –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–π
- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π (–∏–º–µ–Ω–∞, –¥–∞—Ç—ã, –º–µ—Å—Ç–∞)
- –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–≥–æ–≤
- –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AI-—Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   –î–æ–∫—É–º–µ–Ω—Ç      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ     ‚îÇ
‚îÇ  —Ç–µ–∫—Å—Ç–∞         ‚îÇ
‚îÇ  (PDF, DOCX)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞  ‚îÇ
‚îÇ  ‚Ä¢ –û—á–∏—Å—Ç–∫–∞      ‚îÇ
‚îÇ  ‚Ä¢ –†–∞–∑–±–∏–µ–Ω–∏–µ    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI –æ–±—Ä–∞–±–æ—Ç–∫–∞   ‚îÇ
‚îÇ  ‚Ä¢ LLM          ‚îÇ
‚îÇ  ‚Ä¢ NLP          ‚îÇ
‚îÇ  ‚Ä¢ Embeddings   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ     ‚îÇ
‚îÇ  .meta.json     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## –ß–∞—Å—Ç—å 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ OpenAI / Anthropic API

### 1.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
python3 -m venv venv
source venv/bin/activate

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
pip install openai anthropic tiktoken python-dotenv pypdf2 python-docx
```

### 1.2 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–µ–π

```bash
# –°–æ–∑–¥–∞–π—Ç–µ .env —Ñ–∞–π–ª
cat > .env << 'EOF'
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
EOF

# –ù–µ –¥–æ–±–∞–≤–ª—è–π—Ç–µ .env –≤ git!
echo ".env" >> .gitignore
```

### 1.3 –°–æ–∑–¥–∞–Ω–∏–µ AI –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞

```python
# tools/ai_metadata_generator.py
#!/usr/bin/env python3
"""
AI-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, Optional
import anthropic
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


class AIMetadataGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI"""

    def __init__(self, provider="anthropic"):
        self.provider = provider

        if provider == "anthropic":
            self.client = anthropic.Anthropic(
                api_key=os.getenv("ANTHROPIC_API_KEY")
            )
        elif provider == "openai":
            import openai
            openai.api_key = os.getenv("OPENAI_API_KEY")
            self.client = openai

    def extract_text(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞"""

        if file_path.suffix == '.pdf':
            return self._extract_from_pdf(file_path)
        elif file_path.suffix in ['.txt', '.md']:
            return file_path.read_text(encoding='utf-8')
        elif file_path.suffix in ['.docx']:
            return self._extract_from_docx(file_path)
        else:
            return ""

    def _extract_from_pdf(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF"""
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                # –ò–∑–≤–ª–µ—á—å –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
                for page in reader.pages[:10]:
                    text += page.extract_text()
                return text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ PDF: {e}")
            return ""

    def _extract_from_docx(self, file_path: Path) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ DOCX"""
        try:
            import docx
            doc = docx.Document(file_path)
            return "\n".join([para.text for para in doc.paragraphs])
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ DOCX: {e}")
            return ""

    def generate_metadata(self, file_path: Path, text: str) -> Dict[str, Any]:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é AI"""

        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        text_sample = text[:5000]

        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–æ–∑–¥–∞–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.

–î–æ–∫—É–º–µ–Ω—Ç: {file_path.name}

–°–æ–¥–µ—Ä–∂–∏–º–æ–µ (–ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤):
{text_sample}

–°–æ–∑–¥–∞–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø–æ–ª—è–º–∏:
- title: –∫—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (20-50 —Å–ª–æ–≤)
- description: –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- abstract: –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è (50-100 —Å–ª–æ–≤)
- keywords: –º–∞—Å—Å–∏–≤ –∏–∑ 5-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
- tags: –º–∞—Å—Å–∏–≤ –∏–∑ 3-5 —Ç–µ–≥–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
- category: –æ—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è (education, work, science, personal, etc.)
- subject: –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- language: —è–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (ru, en, de, etc.)
- importance: –≤–∞–∂–Ω–æ—Å—Ç—å (critical, high, medium, low)

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π."""

        if self.provider == "anthropic":
            message = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1024,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            response_text = message.content[0].text

        elif self.provider == "openai":
            response = self.client.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."},
                    {"role": "user", "content": prompt}
                ]
            )
            response_text = response.choices[0].message.content

        # –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
        try:
            # –ò–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ markdown –±–ª–æ–∫–µ)
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0]

            metadata = json.loads(response_text.strip())
            return metadata

        except json.JSONDecodeError as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            print(f"–û—Ç–≤–µ—Ç AI: {response_text}")
            return {}

    def generate_summary(self, text: str, max_length: int = 500) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ"""

        text_sample = text[:8000]

        prompt = f"""–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (summary) —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
- –î–ª–∏–Ω–∞: {max_length} —Å–ª–æ–≤
- –§–æ—Ä–º–∞—Ç: —Å–≤—è–∑–Ω—ã–π —Ç–µ–∫—Å—Ç, 2-3 –∞–±–∑–∞—Ü–∞
- –í–∫–ª—é—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ –∏ –≤—ã–≤–æ–¥—ã
- –ü–∏—à–∏ –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –¥–æ–∫—É–º–µ–Ω—Ç

–î–æ–∫—É–º–µ–Ω—Ç:
{text_sample}

–ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:"""

        if self.provider == "anthropic":
            message = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=1024,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return message.content[0].text

        elif self.provider == "openai":
            response = self.client.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content

    def generate_toc(self, text: str) -> list:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ"""

        text_sample = text[:10000]

        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–æ–∑–¥–∞–π –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ.

–î–æ–∫—É–º–µ–Ω—Ç:
{text_sample}

–°–æ–∑–¥–∞–π –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
[
  {{"title": "–†–∞–∑–¥–µ–ª 1", "level": 1, "summary": "–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ"}},
  {{"title": "–ü–æ–¥—Ä–∞–∑–¥–µ–ª 1.1", "level": 2, "summary": "–æ–ø–∏—Å–∞–Ω–∏–µ"}},
  ...
]

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ JSON –º–∞—Å—Å–∏–≤."""

        if self.provider == "anthropic":
            message = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=2048,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            response_text = message.content[0].text
        else:
            response = self.client.ChatCompletion.create(
                model="gpt-4",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            response_text = response.choices[0].message.content

        try:
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            return json.loads(response_text.strip())
        except:
            return []

    def process_document(self, file_path: Path, output_dir: Optional[Path] = None):
        """–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path.name}")

        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
        print("  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...")
        text = self.extract_text(file_path)

        if not text:
            print("  ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç")
            return

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        print("  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö...")
        ai_metadata = self.generate_metadata(file_path, text)

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–µ
        meta_file = file_path.parent / f"{file_path.stem}.meta.json"

        if meta_file.exists():
            with open(meta_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            from datetime import datetime
            metadata = {
                "filename": file_path.name,
                "created": datetime.now().isoformat(),
            }

        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å AI –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        metadata.update(ai_metadata)
        metadata["updated"] = datetime.now().isoformat()
        metadata["ai_generated"] = True

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"  ‚úì –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {meta_file.name}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        print("  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è summary...")
        summary = self.generate_summary(text)
        summary_file = file_path.parent / f"{file_path.stem}.summary.md"

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"# –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {file_path.name}\n\n")
            f.write(summary)
        print(f"  ‚úì Summary: {summary_file.name}")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è
        print("  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è...")
        toc = self.generate_toc(text)

        if toc:
            toc_file = file_path.parent / f"{file_path.stem}.toc.md"
            with open(toc_file, 'w', encoding='utf-8') as f:
                f.write(f"# –û–≥–ª–∞–≤–ª–µ–Ω–∏–µ: {file_path.name}\n\n")
                for item in toc:
                    indent = "  " * (item.get("level", 1) - 1)
                    f.write(f"{indent}- {item['title']}\n")
                    if item.get("summary"):
                        f.write(f"{indent}  {item['summary']}\n")
            print(f"  ‚úì TOC: {toc_file.name}")

        print(f"‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {file_path.name}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="AI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
    )
    parser.add_argument('file', help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É')
    parser.add_argument('--provider', choices=['anthropic', 'openai'],
                        default='anthropic', help='AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä')

    args = parser.parse_args()

    generator = AIMetadataGenerator(provider=args.provider)
    generator.process_document(Path(args.file))


if __name__ == "__main__":
    main()
```

–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —ç—Ç–æ –∫–∞–∫ `/path/to/tools/ai_metadata_generator.py` –∏ —Å–¥–µ–ª–∞–π—Ç–µ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–º.

### 1.4 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ AI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞

```bash
# –î–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
python3 tools/ai_metadata_generator.py document.pdf

# –° OpenAI –≤–º–µ—Å—Ç–æ Anthropic
python3 tools/ai_metadata_generator.py document.pdf --provider openai

# –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
find /path/to/documents -name "*.pdf" | while read file; do
    python3 tools/ai_metadata_generator.py "$file"
done
```

## –ß–∞—Å—Ç—å 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö NLP –º–æ–¥–µ–ª–µ–π

–î–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ –∏ API –∫–ª—é—á–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏.

### 2.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ spaCy

```bash
pip install spacy

# –°–∫–∞—á–∞–π—Ç–µ —Ä—É—Å—Å–∫—É—é –º–æ–¥–µ–ª—å
python3 -m spacy download ru_core_news_lg

# –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
python3 -m spacy download en_core_web_lg
```

### 2.2 NLP –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä

```python
# tools/nlp_metadata_generator.py
#!/usr/bin/env python3
"""
–õ–æ–∫–∞–ª—å–Ω—ã–π NLP –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
"""

import spacy
from pathlib import Path
from collections import Counter
import json


class NLPMetadataGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ NLP –º–æ–¥–µ–ª—è–º–∏"""

    def __init__(self, language="ru"):
        if language == "ru":
            self.nlp = spacy.load("ru_core_news_lg")
        else:
            self.nlp = spacy.load("en_core_web_lg")

    def extract_keywords(self, text: str, top_n: int = 10) -> list:
        """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""

        doc = self.nlp(text)

        # –°–æ–±—Ä–∞—Ç—å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
        keywords = []
        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop:
                keywords.append(token.lemma_.lower())

        # –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—É
        counter = Counter(keywords)
        return [word for word, count in counter.most_common(top_n)]

    def extract_entities(self, text: str) -> dict:
        """–ò–∑–≤–ª–µ—á—å –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏"""

        doc = self.nlp(text)

        entities = {
            "persons": [],
            "organizations": [],
            "locations": [],
            "dates": []
        }

        for ent in doc.ents:
            if ent.label_ == "PER":
                entities["persons"].append(ent.text)
            elif ent.label_ == "ORG":
                entities["organizations"].append(ent.text)
            elif ent.label_ == "LOC":
                entities["locations"].append(ent.text)
            elif ent.label_ == "DATE":
                entities["dates"].append(ent.text)

        # –£–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        for key in entities:
            entities[key] = list(set(entities[key]))

        return entities

    def generate_summary(self, text: str, num_sentences: int = 3) -> str:
        """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""

        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]

        return " ".join(sentences[:num_sentences])

    def detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""

        from langdetect import detect
        try:
            return detect(text)
        except:
            return "unknown"

    def process_document(self, file_path: Path):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"""

        # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ—Ç –∂–µ –º–µ—Ç–æ–¥, —á—Ç–æ –∏ –≤ AI –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ)
        # ...

        text = file_path.read_text(encoding='utf-8')[:10000]

        # –ê–Ω–∞–ª–∏–∑
        keywords = self.extract_keywords(text)
        entities = self.extract_entities(text)
        summary = self.generate_summary(text)
        language = self.detect_language(text)

        # –°–æ–∑–¥–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            "filename": file_path.name,
            "keywords": keywords,
            "entities": entities,
            "abstract": summary,
            "language": language,
            "nlp_generated": True
        }

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
        meta_file = file_path.parent / f"{file_path.stem}.meta.json"
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path.name}")
```

### 2.3 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ langdetect
pip install langdetect

# –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç
python3 tools/nlp_metadata_generator.py document.txt
```

## –ß–∞—Å—Ç—å 3: –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫

### 3.1 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings

```python
# tools/embeddings_generator.py
#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
"""

from sentence_transformers import SentenceTransformer
import numpy as np
import json
from pathlib import Path


class EmbeddingsGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):
        # –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å
        self.model = SentenceTransformer(model_name)

    def generate_embedding(self, text: str) -> np.ndarray:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        return self.model.encode(text)

    def process_document(self, file_path: Path):
        """–°–æ–∑–¥–∞—Ç—å embedding –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""

        # –ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç
        text = file_path.read_text(encoding='utf-8')[:5000]

        # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å embedding
        embedding = self.generate_embedding(text)

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
        embedding_file = file_path.parent / f"{file_path.stem}.embedding.npy"
        np.save(embedding_file, embedding)

        # –î–æ–±–∞–≤–∏—Ç—å –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        meta_file = file_path.parent / f"{file_path.stem}.meta.json"
        if meta_file.exists():
            with open(meta_file, 'r') as f:
                metadata = json.load(f)

            metadata["embedding_file"] = str(embedding_file.name)
            metadata["embedding_model"] = "paraphrase-multilingual-MiniLM-L12-v2"

            with open(meta_file, 'w') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"‚úì Embedding —Å–æ–∑–¥–∞–Ω: {file_path.name}")

    def find_similar(self, query: str, documents_dir: Path, top_k: int = 5):
        """–ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""

        query_embedding = self.generate_embedding(query)

        similarities = []

        for embedding_file in documents_dir.rglob("*.embedding.npy"):
            doc_embedding = np.load(embedding_file)

            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )

            similarities.append({
                "file": embedding_file.stem,
                "similarity": float(similarity)
            })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
        similarities.sort(key=lambda x: x["similarity"], reverse=True)

        return similarities[:top_k]


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    gen = EmbeddingsGenerator()

    # –°–æ–∑–¥–∞—Ç—å embeddings
    gen.process_document(Path("document.txt"))

    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö
    results = gen.find_similar("–∫–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞", Path("."))
    for result in results:
        print(f"{result['file']}: {result['similarity']:.3f}")
```

### 3.2 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å sentence-transformers
pip install sentence-transformers

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings
python3 tools/embeddings_generator.py

# –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
python3 tools/embeddings_generator.py --query "–∫–≤–∞–Ω—Ç–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞" --search
```

## –ß–∞—Å—Ç—å 4: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –æ—á–µ—Ä–µ–¥–∏

### 4.1 –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –æ—á–µ—Ä–µ–¥–µ–π

```python
# tools/batch_processor.py
#!/usr/bin/env python3
"""
–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—á–µ—Ä–µ–¥—è–º–∏
"""

import asyncio
from pathlib import Path
from queue import Queue
from threading import Thread
from ai_metadata_generator import AIMetadataGenerator


class BatchProcessor:
    """–ü–∞–∫–µ—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def __init__(self, num_workers: int = 3):
        self.queue = Queue()
        self.num_workers = num_workers
        self.generator = AIMetadataGenerator()

    def add_documents(self, directory: Path, pattern: str = "*.pdf"):
        """–î–æ–±–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—á–µ—Ä–µ–¥—å"""

        for file_path in directory.rglob(pattern):
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ—Ç –ª–∏ —É–∂–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            meta_file = file_path.parent / f"{file_path.stem}.meta.json"

            if not meta_file.exists():
                self.queue.put(file_path)

        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {self.queue.qsize()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    def worker(self):
        """–†–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å"""

        while True:
            try:
                file_path = self.queue.get(timeout=1)

                try:
                    self.generator.process_document(file_path)
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {file_path}: {e}")

                self.queue.task_done()

            except:
                break

    def process_all(self):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—á–µ—Ä–µ–¥–∏"""

        # –ó–∞–ø—É—Å—Ç–∏—Ç—å workers
        threads = []
        for i in range(self.num_workers):
            t = Thread(target=self.worker)
            t.start()
            threads.append(t)

        # –î–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        self.queue.join()

        # –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å workers
        for t in threads:
            t.join(timeout=1)

        print("‚úì –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")


# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    processor = BatchProcessor(num_workers=3)
    processor.add_documents(Path("/path/to/documents"), "*.pdf")
    processor.process_all()
```

## –ß–∞—Å—Ç—å 5: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏

### 5.1 –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ crawler.py

–î–æ–±–∞–≤—å—Ç–µ AI –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π crawler:

```python
# –í crawler.py –¥–æ–±–∞–≤—å—Ç–µ:

def process_with_ai(self, file_path: Path):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª —Å AI –µ—Å–ª–∏ –Ω–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""

    meta_file = file_path.parent / f"{file_path.stem}.meta.json"

    if not meta_file.exists():
        try:
            from ai_metadata_generator import AIMetadataGenerator
            generator = AIMetadataGenerator()
            generator.process_document(file_path)
        except Exception as e:
            print(f"AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
```

## –ß–∞—Å—Ç—å 6: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ—Ç—á–µ—Ç—ã

### 6.1 –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏

```python
# tools/processing_dashboard.py
#!/usr/bin/env python3
"""
–î–∞—à–±–æ—Ä–¥ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
"""

import json
from pathlib import Path
from datetime import datetime


def generate_report(documents_dir: Path):
    """–°–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç –æ–± –æ–±—Ä–∞–±–æ—Ç–∫–µ"""

    total_files = 0
    ai_generated = 0
    nlp_generated = 0
    manual = 0
    no_metadata = 0

    for file in documents_dir.rglob("*"):
        if file.is_file() and file.suffix in ['.pdf', '.txt', '.docx']:
            total_files += 1

            meta_file = file.parent / f"{file.stem}.meta.json"

            if meta_file.exists():
                with open(meta_file, 'r') as f:
                    metadata = json.load(f)

                if metadata.get("ai_generated"):
                    ai_generated += 1
                elif metadata.get("nlp_generated"):
                    nlp_generated += 1
                else:
                    manual += 1
            else:
                no_metadata += 1

    report = {
        "generated_at": datetime.now().isoformat(),
        "total_files": total_files,
        "ai_generated": ai_generated,
        "nlp_generated": nlp_generated,
        "manual": manual,
        "no_metadata": no_metadata,
        "completion_rate": (total_files - no_metadata) / total_files * 100 if total_files > 0 else 0
    }

    print(f"üìä –û—Ç—á–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"  –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
    print(f"  AI –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {ai_generated}")
    print(f"  NLP –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {nlp_generated}")
    print(f"  –†—É—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {manual}")
    print(f"  –ë–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {no_metadata}")
    print(f"  –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å: {report['completion_rate']:.1f}%")

    return report
```

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–° AI –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–µ–π –≤—ã –º–æ–∂–µ—Ç–µ:
- –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç—ã—Å—è—á–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- –ü–æ–ª—É—á–∏—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
- –°–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
- –°—ç–∫–æ–Ω–æ–º–∏—Ç—å —á–∞—Å—ã —Ä—É—á–Ω–æ–π —Ä–∞–±–æ—Ç—ã

–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥: —Å–æ–∑–¥–∞–π—Ç–µ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (—Å–º. WEB_INTERFACE.md) –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ (—Å–º. SEARCH_INTEGRATION.md).
