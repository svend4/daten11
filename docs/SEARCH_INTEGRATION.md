# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏

## –í–≤–µ–¥–µ–Ω–∏–µ

–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å–∏—Å—Ç–µ–º—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –º–æ—â–Ω—ã–º–∏ –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ Elasticsearch –∏ Meilisearch –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –±–æ–ª—å—à–∏–º –∫–æ–ª–ª–µ–∫—Ü–∏—è–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º

| –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ | Elasticsearch | Meilisearch |
|---------------|--------------|-------------|
| –°–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ | –°—Ä–µ–¥–Ω—è—è | –ù–∏–∑–∫–∞—è |
| –°–∫–æ—Ä–æ—Å—Ç—å | –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è | –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è |
| –†–∞–∑–º–µ—Ä –∏–Ω–¥–µ–∫—Å–∞ | –ë–æ–ª—å—à–æ–π | –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π |
| –ü–æ–∏—Å–∫ —Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏ | –î–∞ | –î–∞ (–ª—É—á—à–µ) |
| –§–∞—Å–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ | –î–∞ | –î–∞ |
| –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ | –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ | –û—Ç–ª–∏—á–Ω–æ –∏–∑ –∫–æ—Ä–æ–±–∫–∏ |
| –†–µ—Å—É—Ä—Å—ã | –ú–Ω–æ–≥–æ (Java) | –ú–∞–ª–æ (Rust) |
| –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è | –î–ª—è –±–æ–ª—å—à–∏—Ö —Å–∏—Å—Ç–µ–º | –î–ª—è –º–∞–ª—ã—Ö/—Å—Ä–µ–¥–Ω–∏—Ö |

## –ß–∞—Å—Ç—å 1: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Elasticsearch

### 1.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Elasticsearch

#### Docker —Å–ø–æ—Å–æ–± (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

```bash
# –°–æ–∑–¥–∞–π—Ç–µ docker-compose.yml
cat > docker-compose-elasticsearch.yml << 'EOF'
version: '3'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: metadata-elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

volumes:
  elasticsearch-data:
EOF

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ
docker-compose -f docker-compose-elasticsearch.yml up -d

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å
curl http://localhost:9200
```

#### –õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# –î–ª—è Ubuntu/Debian
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
sudo apt update && sudo apt install elasticsearch

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ
sudo systemctl start elasticsearch
sudo systemctl enable elasticsearch
```

### 1.2 Python –∫–ª–∏–µ–Ω—Ç

```bash
pip install elasticsearch
```

### 1.3 –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ –¥–ª—è Elasticsearch

```python
# tools/elasticsearch_indexer.py
#!/usr/bin/env python3
"""
–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ Elasticsearch
"""

from elasticsearch import Elasticsearch
from pathlib import Path
import json
from datetime import datetime
from typing import Dict, Any


class ElasticsearchIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–ª—è Elasticsearch"""

    def __init__(self, host="localhost", port=9200):
        self.es = Elasticsearch([f"http://{host}:{port}"])
        self.index_name = "documents_metadata"

    def create_index(self):
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å —Å –º–∞–ø–ø–∏–Ω–≥–æ–º"""

        mapping = {
            "settings": {
                "analysis": {
                    "analyzer": {
                        "russian": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": [
                                "lowercase",
                                "russian_stop",
                                "russian_stemmer"
                            ]
                        }
                    },
                    "filter": {
                        "russian_stop": {
                            "type": "stop",
                            "stopwords": "_russian_"
                        },
                        "russian_stemmer": {
                            "type": "stemmer",
                            "language": "russian"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "filename": {
                        "type": "keyword"
                    },
                    "title": {
                        "type": "text",
                        "analyzer": "russian",
                        "fields": {
                            "keyword": {
                                "type": "keyword"
                            }
                        }
                    },
                    "description": {
                        "type": "text",
                        "analyzer": "russian"
                    },
                    "abstract": {
                        "type": "text",
                        "analyzer": "russian"
                    },
                    "content": {
                        "type": "text",
                        "analyzer": "russian"
                    },
                    "fileType": {
                        "type": "keyword"
                    },
                    "category": {
                        "type": "keyword"
                    },
                    "tags": {
                        "type": "keyword"
                    },
                    "keywords": {
                        "type": "keyword"
                    },
                    "author": {
                        "type": "keyword"
                    },
                    "language": {
                        "type": "keyword"
                    },
                    "created": {
                        "type": "date"
                    },
                    "updated": {
                        "type": "date"
                    },
                    "size": {
                        "type": "long"
                    },
                    "importance": {
                        "type": "keyword"
                    },
                    "status": {
                        "type": "keyword"
                    },
                    "file_path": {
                        "type": "keyword"
                    },
                    "folder_path": {
                        "type": "keyword"
                    }
                }
            }
        }

        # –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if self.es.indices.exists(index=self.index_name):
            self.es.indices.delete(index=self.index_name)

        # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        self.es.indices.create(index=self.index_name, body=mapping)
        print(f"‚úì –ò–Ω–¥–µ–∫—Å '{self.index_name}' —Å–æ–∑–¥–∞–Ω")

    def index_document(self, file_path: Path, metadata: Dict[str, Any]):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"""

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        doc = {
            "filename": metadata.get("filename", file_path.name),
            "title": metadata.get("title", ""),
            "description": metadata.get("description", ""),
            "abstract": metadata.get("abstract", ""),
            "fileType": metadata.get("fileType", ""),
            "category": metadata.get("category", ""),
            "tags": metadata.get("tags", []),
            "keywords": metadata.get("keywords", []),
            "author": metadata.get("author", ""),
            "language": metadata.get("language", "ru"),
            "created": metadata.get("created", datetime.now().isoformat()),
            "updated": metadata.get("updated", datetime.now().isoformat()),
            "size": metadata.get("size", 0),
            "importance": metadata.get("importance", "medium"),
            "status": metadata.get("status", "final"),
            "file_path": str(file_path),
            "folder_path": str(file_path.parent)
        }

        # –î–æ–±–∞–≤–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        summary_file = file_path.parent / f"{file_path.stem}.summary.md"
        if summary_file.exists():
            doc["content"] = summary_file.read_text(encoding='utf-8')

        # –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å
        doc_id = str(file_path).replace("/", "_")

        self.es.index(
            index=self.index_name,
            id=doc_id,
            document=doc
        )

    def index_collection(self, base_path: Path):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é"""

        count = 0

        print(f"üî® –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {base_path}")

        for meta_file in base_path.rglob("*.meta.json"):
            if meta_file.name.startswith('.folder-'):
                continue

            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                file_path = meta_file.parent / metadata['filename']

                self.index_document(file_path, metadata)
                count += 1

                if count % 100 == 0:
                    print(f"  –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

            except Exception as e:
                print(f"  ‚ö† –û—à–∏–±–∫–∞ {meta_file}: {e}")

        print(f"‚úì –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        # –û–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å
        self.es.indices.refresh(index=self.index_name)

    def search(self, query: str, filters: Dict[str, Any] = None, size: int = 10):
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

        # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–ø—Ä–æ—Å
        must_clauses = []

        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –ø–æ–ª—è–º
        if query:
            must_clauses.append({
                "multi_match": {
                    "query": query,
                    "fields": [
                        "title^3",
                        "description^2",
                        "abstract^2",
                        "keywords^2",
                        "content",
                        "tags"
                    ],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            })

        # –§–∏–ª—å—Ç—Ä—ã
        filter_clauses = []

        if filters:
            if filters.get("category"):
                filter_clauses.append({
                    "term": {"category": filters["category"]}
                })

            if filters.get("fileType"):
                filter_clauses.append({
                    "term": {"fileType": filters["fileType"]}
                })

            if filters.get("tags"):
                filter_clauses.append({
                    "terms": {"tags": filters["tags"]}
                })

            if filters.get("author"):
                filter_clauses.append({
                    "term": {"author": filters["author"]}
                })

        # –°–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        body = {
            "query": {
                "bool": {
                    "must": must_clauses if must_clauses else [{"match_all": {}}],
                    "filter": filter_clauses
                }
            },
            "size": size,
            "highlight": {
                "fields": {
                    "title": {},
                    "description": {},
                    "content": {}
                }
            }
        }

        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫
        response = self.es.search(index=self.index_name, body=body)

        return response

    def get_aggregations(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ (—Ñ–∞—Å–µ—Ç—ã)"""

        body = {
            "size": 0,
            "aggs": {
                "by_category": {
                    "terms": {"field": "category", "size": 20}
                },
                "by_type": {
                    "terms": {"field": "fileType", "size": 20}
                },
                "by_tags": {
                    "terms": {"field": "tags", "size": 50}
                },
                "by_author": {
                    "terms": {"field": "author", "size": 20}
                }
            }
        }

        response = self.es.search(index=self.index_name, body=body)

        return response["aggregations"]


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä Elasticsearch"
    )

    parser.add_argument('--create-index', action='store_true',
                        help='–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å')
    parser.add_argument('--index-collection', type=str,
                        help='–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é')
    parser.add_argument('--search', type=str,
                        help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å')
    parser.add_argument('--category', type=str,
                        help='–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
    parser.add_argument('--type', type=str,
                        help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É')

    args = parser.parse_args()

    indexer = ElasticsearchIndexer()

    if args.create_index:
        indexer.create_index()

    if args.index_collection:
        indexer.index_collection(Path(args.index_collection))

    if args.search:
        filters = {}
        if args.category:
            filters['category'] = args.category
        if args.type:
            filters['fileType'] = args.type

        results = indexer.search(args.search, filters=filters)

        print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ: {results['hits']['total']['value']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n")

        for hit in results['hits']['hits']:
            source = hit['_source']
            score = hit['_score']

            print(f"üìÑ {source['title']} (score: {score:.2f})")
            print(f"   {source['description'][:100]}...")
            print(f"   –ü—É—Ç—å: {source['file_path']}")

            if 'highlight' in hit:
                for field, highlights in hit['highlight'].items():
                    print(f"   üí° {field}: {highlights[0]}")

            print()


if __name__ == "__main__":
    main()
```

### 1.4 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Elasticsearch

```bash
# –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
python3 tools/elasticsearch_indexer.py --create-index

# –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é
python3 tools/elasticsearch_indexer.py --index-collection /path/to/documents

# –ü–æ–∏—Å–∫
python3 tools/elasticsearch_indexer.py --search "–∫–≤–∞–Ω—Ç–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞"

# –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
python3 tools/elasticsearch_indexer.py --search "—Ñ–∏–∑–∏–∫–∞" --category "science" --type "pdf"
```

## –ß–∞—Å—Ç—å 2: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Meilisearch

### 2.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Meilisearch

#### Docker —Å–ø–æ—Å–æ–±

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ Meilisearch
docker run -d \
  --name meilisearch \
  -p 7700:7700 \
  -v $(pwd)/meili_data:/meili_data \
  getmeili/meilisearch:v1.5

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å
curl http://localhost:7700/health
```

#### –õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
# –°–∫–∞—á–∞–π—Ç–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ
curl -L https://install.meilisearch.com | sh

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ
./meilisearch

# –ò–ª–∏ –∫–∞–∫ —Å–µ—Ä–≤–∏—Å
sudo systemctl start meilisearch
```

### 2.2 Python –∫–ª–∏–µ–Ω—Ç

```bash
pip install meilisearch
```

### 2.3 –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ –¥–ª—è Meilisearch

```python
# tools/meilisearch_indexer.py
#!/usr/bin/env python3
"""
–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ Meilisearch
"""

import meilisearch
from pathlib import Path
import json
from typing import Dict, Any, List


class MeilisearchIndexer:
    """–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –¥–ª—è Meilisearch"""

    def __init__(self, host="http://localhost:7700", api_key=None):
        self.client = meilisearch.Client(host, api_key)
        self.index_name = "documents"

    def create_index(self):
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""

        try:
            # –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
            index = self.client.create_index(
                self.index_name,
                {'primaryKey': 'id'}
            )

            # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å searchable attributes
            self.client.index(self.index_name).update_searchable_attributes([
                'title',
                'description',
                'abstract',
                'keywords',
                'tags',
                'content',
                'author'
            ])

            # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å filterable attributes
            self.client.index(self.index_name).update_filterable_attributes([
                'category',
                'fileType',
                'tags',
                'author',
                'language',
                'importance',
                'status'
            ])

            # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å sortable attributes
            self.client.index(self.index_name).update_sortable_attributes([
                'created',
                'updated',
                'size',
                'title'
            ])

            # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å ranking rules
            self.client.index(self.index_name).update_ranking_rules([
                'words',
                'typo',
                'proximity',
                'attribute',
                'sort',
                'exactness'
            ])

            print(f"‚úì –ò–Ω–¥–µ–∫—Å '{self.index_name}' —Å–æ–∑–¥–∞–Ω")

        except Exception as e:
            print(f"–ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {e}")

    def index_document(self, file_path: Path, metadata: Dict[str, Any]):
        """–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"""

        # –°–æ–∑–¥–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        doc_id = str(file_path).replace("/", "_")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
        doc = {
            "id": doc_id,
            "filename": metadata.get("filename", file_path.name),
            "title": metadata.get("title", ""),
            "description": metadata.get("description", ""),
            "abstract": metadata.get("abstract", ""),
            "fileType": metadata.get("fileType", ""),
            "category": metadata.get("category", ""),
            "tags": metadata.get("tags", []),
            "keywords": metadata.get("keywords", []),
            "author": metadata.get("author", ""),
            "language": metadata.get("language", "ru"),
            "created": metadata.get("created", ""),
            "updated": metadata.get("updated", ""),
            "size": metadata.get("size", 0),
            "importance": metadata.get("importance", "medium"),
            "status": metadata.get("status", "final"),
            "file_path": str(file_path),
            "folder_path": str(file_path.parent)
        }

        # –î–æ–±–∞–≤–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        summary_file = file_path.parent / f"{file_path.stem}.summary.md"
        if summary_file.exists():
            doc["content"] = summary_file.read_text(encoding='utf-8')[:5000]

        return doc

    def index_collection(self, base_path: Path):
        """–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é"""

        documents = []
        count = 0

        print(f"üî® –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {base_path}")

        for meta_file in base_path.rglob("*.meta.json"):
            if meta_file.name.startswith('.folder-'):
                continue

            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                file_path = meta_file.parent / metadata['filename']

                doc = self.index_document(file_path, metadata)
                documents.append(doc)
                count += 1

                # –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø–∞—á–∫–∞–º–∏ –ø–æ 100
                if len(documents) >= 100:
                    self.client.index(self.index_name).add_documents(documents)
                    documents = []
                    print(f"  –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

            except Exception as e:
                print(f"  ‚ö† –û—à–∏–±–∫–∞ {meta_file}: {e}")

        # –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –æ—Å—Ç–∞–≤—à–∏–µ—Å—è
        if documents:
            self.client.index(self.index_name).add_documents(documents)

        print(f"‚úì –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    def search(self, query: str, filters: Dict[str, Any] = None, limit: int = 20):
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
        filter_str = None

        if filters:
            filter_parts = []

            if filters.get("category"):
                filter_parts.append(f'category = "{filters["category"]}"')

            if filters.get("fileType"):
                filter_parts.append(f'fileType = "{filters["fileType"]}"')

            if filters.get("tags"):
                tags_filter = " OR ".join([f'tags = "{tag}"' for tag in filters["tags"]])
                filter_parts.append(f"({tags_filter})")

            if filter_parts:
                filter_str = " AND ".join(filter_parts)

        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫
        results = self.client.index(self.index_name).search(
            query,
            {
                'limit': limit,
                'filter': filter_str,
                'attributesToHighlight': ['title', 'description', 'content'],
                'highlightPreTag': '<mark>',
                'highlightPostTag': '</mark>'
            }
        )

        return results

    def get_facets(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ñ–∞—Å–µ—Ç—ã"""

        # Meilisearch —Ç—Ä–µ–±—É–µ—Ç –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ñ–∞—Å–µ—Ç–æ–≤
        results = self.client.index(self.index_name).search(
            "",
            {
                'facets': ['category', 'fileType', 'tags', 'author']
            }
        )

        return results.get('facetDistribution', {})


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="–ò–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä Meilisearch"
    )

    parser.add_argument('--create-index', action='store_true',
                        help='–°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å')
    parser.add_argument('--index-collection', type=str,
                        help='–ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é')
    parser.add_argument('--search', type=str,
                        help='–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å')
    parser.add_argument('--category', type=str,
                        help='–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏')
    parser.add_argument('--type', type=str,
                        help='–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É')

    args = parser.parse_args()

    indexer = MeilisearchIndexer()

    if args.create_index:
        indexer.create_index()

    if args.index_collection:
        indexer.index_collection(Path(args.index_collection))

    if args.search:
        filters = {}
        if args.category:
            filters['category'] = args.category
        if args.type:
            filters['fileType'] = args.type

        results = indexer.search(args.search, filters=filters)

        print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ: {results['estimatedTotalHits']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print(f"‚è± –í—Ä–µ–º—è –ø–æ–∏—Å–∫–∞: {results['processingTimeMs']} –º—Å\n")

        for hit in results['hits']:
            print(f"üìÑ {hit['title']}")
            print(f"   {hit['description'][:100]}...")
            print(f"   –ü—É—Ç—å: {hit['file_path']}")

            if '_formatted' in hit:
                if hit['_formatted'].get('title') != hit['title']:
                    print(f"   üí° {hit['_formatted']['title']}")

            print()


if __name__ == "__main__":
    main()
```

### 2.4 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Meilisearch

```bash
# –°–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å
python3 tools/meilisearch_indexer.py --create-index

# –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é
python3 tools/meilisearch_indexer.py --index-collection /path/to/documents

# –ü–æ–∏—Å–∫ (—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –æ–ø–µ—á–∞—Ç–æ–∫!)
python3 tools/meilisearch_indexer.py --search "–∫–≤–∞–Ω—Ç–æ –≤–∞—è –º–µ—Ö–∞–Ω–∫–∞"

# –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
python3 tools/meilisearch_indexer.py --search "—Ñ–∏–∑–∏–∫–∞" --category "science"
```

## –ß–∞—Å—Ç—å 3: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤

### 3.1 Watchdog –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π

```python
# tools/index_watcher.py
#!/usr/bin/env python3
"""
–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
"""

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from pathlib import Path
import json
import time


class MetadataHandler(FileSystemEventHandler):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self, indexer):
        self.indexer = indexer

    def on_created(self, event):
        if event.src_path.endswith('.meta.json'):
            self.update_index(event.src_path)

    def on_modified(self, event):
        if event.src_path.endswith('.meta.json'):
            self.update_index(event.src_path)

    def update_index(self, meta_path):
        """–û–±–Ω–æ–≤–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è —Ñ–∞–π–ª–∞"""

        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            file_path = Path(meta_path).parent / metadata['filename']

            self.indexer.index_document(file_path, metadata)
            print(f"‚úì –û–±–Ω–æ–≤–ª–µ–Ω –∏–Ω–¥–µ–∫—Å –¥–ª—è: {file_path.name}")

        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")


def watch_directory(documents_path, indexer_type="meilisearch"):
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é"""

    if indexer_type == "meilisearch":
        from meilisearch_indexer import MeilisearchIndexer
        indexer = MeilisearchIndexer()
    else:
        from elasticsearch_indexer import ElasticsearchIndexer
        indexer = ElasticsearchIndexer()

    event_handler = MetadataHandler(indexer)
    observer = Observer()
    observer.schedule(event_handler, documents_path, recursive=True)
    observer.start()

    print(f"üëÄ –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤: {documents_path}")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()

    observer.join()


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python3 index_watcher.py /path/to/documents [meilisearch|elasticsearch]")
        sys.exit(1)

    docs_path = sys.argv[1]
    indexer_type = sys.argv[2] if len(sys.argv) > 2 else "meilisearch"

    watch_directory(docs_path, indexer_type)
```

### 3.2 –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ watchdog
pip install watchdog

# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ
python3 tools/index_watcher.py /path/to/documents meilisearch
```

## –ß–∞—Å—Ç—å 4: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º

–û–±–Ω–æ–≤–∏—Ç–µ `web/app.py` –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Å–∏—Å—Ç–µ–º:

```python
# –î–æ–±–∞–≤—å—Ç–µ –≤ web/app.py

from meilisearch_indexer import MeilisearchIndexer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
meilisearch = MeilisearchIndexer()


@app.route('/api/search/meili')
def api_search_meili():
    """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Meilisearch"""

    query = request.args.get('q', '')
    category = request.args.get('category')
    file_type = request.args.get('type')

    filters = {}
    if category:
        filters['category'] = category
    if file_type:
        filters['fileType'] = file_type

    results = meilisearch.search(query, filters=filters)

    return jsonify(results)
```

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –º–æ—â–Ω–∞—è –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞:

- **Elasticsearch** –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π –∏ —Å–ª–æ–∂–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
- **Meilisearch** –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏ –ø—Ä–æ—Å—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å –æ—Ç–ª–∏—á–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –æ–ø–µ—á–∞—Ç–æ–∫
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º

–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
1. –î–æ–±–∞–≤—å—Ç–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (vector search)
2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
3. –î–æ–±–∞–≤—å—Ç–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
4. –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
